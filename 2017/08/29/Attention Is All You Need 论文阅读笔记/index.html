<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.5.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Abstract目前主流的处理序列问题都是使用复杂的循环神经网络或者卷积神经网络作为encoder和decoder，添加注意力机制能够使得其表现更好。这篇文章提出来一个新的网络结构，Transformer，仅仅只依赖注意力机制，完全没有使用循环结构和卷积结构。">
<meta name="keywords" content="Paper">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Is All You Need 论文阅读笔记">
<meta property="og:url" content="https://l1aoxingyu.github.io/2017/08/29/Attention Is All You Need 论文阅读笔记/index.html">
<meta property="og:site_name" content="LiaoXingyu&#39;s Blog">
<meta property="og:description" content="Abstract目前主流的处理序列问题都是使用复杂的循环神经网络或者卷积神经网络作为encoder和decoder，添加注意力机制能够使得其表现更好。这篇文章提出来一个新的网络结构，Transformer，仅仅只依赖注意力机制，完全没有使用循环结构和卷积结构。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/3623720-e68605d38bd802f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/3623720-5319565e9e55b635.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-08-30T13:41:50.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attention Is All You Need 论文阅读笔记">
<meta name="twitter:description" content="Abstract目前主流的处理序列问题都是使用复杂的循环神经网络或者卷积神经网络作为encoder和decoder，添加注意力机制能够使得其表现更好。这篇文章提出来一个新的网络结构，Transformer，仅仅只依赖注意力机制，完全没有使用循环结构和卷积结构。">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/3623720-e68605d38bd802f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">






  <link rel="canonical" href="https://l1aoxingyu.github.io/2017/08/29/Attention Is All You Need 论文阅读笔记/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Attention Is All You Need 论文阅读笔记 | LiaoXingyu's Blog</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LiaoXingyu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">廖星宇的博客</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://l1aoxingyu.github.io/2017/08/29/Attention Is All You Need 论文阅读笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="LiaoXingyu">
      <meta itemprop="description" content="关于计算机视觉，深度学习和机器学习 | LiaoXingyu，Deep Learner & 健身爱好者 | 这里是 @xingyu 的个人博客，I want to create some new things!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiaoXingyu's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Attention Is All You Need 论文阅读笔记
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-08-29 19:37:10" itemprop="dateCreated datePublished" datetime="2017-08-29T19:37:10+08:00">2017-08-29</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2017-08-30 21:41:50" itemprop="dateModified" datetime="2017-08-30T21:41:50+08:00">2017-08-30</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>目前主流的处理序列问题都是使用复杂的循环神经网络或者卷积神经网络作为encoder和decoder，添加注意力机制能够使得其表现更好。这篇文章提出来一个新的网络结构，Transformer，仅仅只依赖注意力机制，完全没有使用循环结构和卷积结构。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>循环神经网络现在已经达到了state of art的水平，但是其每个时间步的输出需要依赖于前面时间步的输出，这使得模型没有办法并行。</p>
<p>注意力机制(attention mechanism)变成了整个网络结构中的重要部分，这能够让模型依赖于特定的输入序列而不需要管他们之间的距离。</p>
<p>在Transfomer新结构中，完全依赖于注意力机制，同时使得任务能够并行。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>为了减少序列的运算时间，已经有很多网络做出了改进，比如Extended Neural GPU，ByteNet和ConvS2S等。但是对于任意长度的输入和输出，这些模型都需要进行很多步操作来获取其中的依赖关系，也就是注意力机制。在Transformer模型中，我们只需要一个常数水平的操作量就能够处理注意力机制，这主要依赖于多头注意力机制(Multi-Head Attention)在位置上的加权平均。</p>
<p>自我注意力(self-attention or intra-attention)会关注序列的位置关系来计算这个序列的表示，在阅读理解，摘要总结等任务中已经取得了成功。</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>目前取得成功的序列模型都是encoder-decoder结构，encoder将输入序列($x_1$, $\cdots$, $x_n$)映射到相同长度的序列($z_1$, $\cdots$, $z_n$)，decoder会将这个序列隐射到最后的输出序列($y_1$, $\cdots$, $y_m$)。</p>
<p>Transformer也会遵循这种结构，encoder和decoder都使用堆叠的self-attention和point-wise，fully connected layers，可以用下面的图片表示。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3623720-e68605d38bd802f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<h3 id="Encoder和Decoder"><a href="#Encoder和Decoder" class="headerlink" title="Encoder和Decoder"></a>Encoder和Decoder</h3><p><strong>Encoder:</strong> encoder由6个相同的层堆叠而成，每个层有两个子层。第一个子层是多头自我注意力机制(multi-head self-attention mechanism)，第二层是简单的位置的全连接前馈网络(position-wise fully connected feed-forward network)。在两个子层中会使用一个残差连接，接着进行层标准化(layer normalization)。也就是说每一个子层的输出都是LayerNorm(x + sublayer(x))。网络输入是三个相同的向量q, k和v，是word embedding和position embedding相加得到的结果。为了方便进行残差连接，我们需要子层的输出和输入都是相同的维度$d_{model} = 512$。</p>
<p><strong>Decoder:</strong> decoder也由6个相同的层堆叠而成，除了有着encoder中相同的两个子层，还有第三个子层，对encoder的输出做多头注意力(multi-head attention)。decoder中也有残差连接和Layer Normalizaiton。对于decoder中的第一个多头注意力子层，需要添加masking，确保预测位置i的时候仅仅依赖于位置小于i的输出。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>注意力机制可以描述为一个函数，这个函数将query和一组key-value对映射成一个输出。</p>
<h3 id="Scaled-Dot-Porduct-Attention"><a href="#Scaled-Dot-Porduct-Attention" class="headerlink" title="Scaled Dot-Porduct Attention"></a>Scaled Dot-Porduct Attention</h3><p>这是论文提出的注意力机制，如下图左边所示。输入的queries和keys的维度是$d_k$，values的维度是$d_v$。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3623720-5319565e9e55b635.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>首先计算q和k的点乘，然后除以$\sqrt{d_k}$，经过softmax得到v上的权重分布，最后通过点乘计算v的加权值。</p>
<p>在实际中为了并行计算，可以在一组queries上计算注意力函数，将多个query堆叠成Q，同理keys和values也被堆叠成K和V，通过下面的公式来计算矩阵输出:</p>
<script type="math/tex; mode=display">
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>有两种常用的注意力函数，一种是加法注意力(additive attention)，另外一种是点乘注意力(dot-product attention)，论文所采用的就是点乘注意力，这种注意力机制对于加法注意力而言，更快，同时更节省空间。</p>
<p>如果$d_k$比较小，那么两种注意力机制差别不大，但是如果$d_k$太大，点乘的值太大，如果不做scaling，结果就没有加法注意力好。另外，点乘的结果过大，这使得经过softmax之后的梯度很小，不利于反向传播的进行，所以我们通过对点乘的结果进行尺度化$\frac{1}{\sqrt{d_k}}$。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>除了计算一个单独的注意力函数，论文提出对queries，keys和values做h次不同的投影，映射的维度都是$d_k$, $d_k$和$d_v$，然后都经过Scaled Dot-Product Attention，将结果拼接在一起，最后通过一个线性映射输出，通过多头注意力，模型能够获得不同子空间下的位置信息。如上图右边所示，公式如下:</p>
<script type="math/tex; mode=display">
MultiHead(Q, K, V) = Concat(head_1, \cdots head_h)W^o</script><script type="math/tex; mode=display">
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>在实验中，h=8, $d<em>k$ = $d_v$ = $d</em>{model}$ / h = 64。</p>
<h3 id="Appications-of-Attention-in-our-Model"><a href="#Appications-of-Attention-in-our-Model" class="headerlink" title="Appications of Attention in our Model"></a>Appications of Attention in our Model</h3><p>Transformer会在三个不同的方面使用multi-head attention：</p>
<ul>
<li><p>在decoder中可以attend输入序列的每个位置</p>
</li>
<li><p>在encoder的self-attention层中，keys，values和queries都来自相同的地方</p>
</li>
<li><p>在decoder的self-attention层中，deocder都能够访问当前位置前面的位置</p>
</li>
</ul>
<h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>包含两个线性变化和ReLU激活函数</p>
<script type="math/tex; mode=display">
FFN(x) = max(0, xW_1 + b_1) W_2 + b_2</script><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>跟一般的序列模型一样，我们会使用context embedding和position embedding。</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>因为模型中没有循环和卷积结构，为了使用序列的顺序，我们必须要将位置信息记录，进行位置编码有很多选择，可以选用可学习的参数，也可以选择固定的，这里论文使用了下面的方法进行位置的编码。</p>
<script type="math/tex; mode=display">
PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">
PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})</script><h2 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h2><p>第一个优点是每层的计算复杂度，第二个优点是可以并行，第三个优点是可以学习更长范围的依赖。</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>以下是论文的实现，有三个版本</p>
<ul>
<li><p><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="external">tensorflow</a></p>
</li>
<li><p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch" target="_blank" rel="external">PyTorch</a></p>
</li>
<li><p><a href="https://github.com/soskek/attention_is_all_you_need" target="_blank" rel="external">Chainer</a></p>
</li>
</ul>

      
    </div>

    

    
    
    

    

    
       
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/24/MXNet的动态图接口Gluon/" rel="next" title="MXNet的动态图接口Gluon">
                <i class="fa fa-chevron-left"></i> MXNet的动态图接口Gluon
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/30/cs20si3/" rel="prev" title="cs20si：tensorflow for research 学习笔记3">
                cs20si：tensorflow for research 学习笔记3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">LiaoXingyu</p>
              <p class="site-description motion-element" itemprop="description">关于计算机视觉，深度学习和机器学习 | LiaoXingyu，Deep Learner & 健身爱好者 | 这里是 @xingyu 的个人博客，I want to create some new things!</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">47</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">3.</span> <span class="nav-text">Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Architecture"><span class="nav-number">4.</span> <span class="nav-text">Model Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder和Decoder"><span class="nav-number">4.1.</span> <span class="nav-text">Encoder和Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-number">4.2.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaled-Dot-Porduct-Attention"><span class="nav-number">4.3.</span> <span class="nav-text">Scaled Dot-Porduct Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Head-Attention"><span class="nav-number">4.4.</span> <span class="nav-text">Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Appications-of-Attention-in-our-Model"><span class="nav-number">4.5.</span> <span class="nav-text">Appications of Attention in our Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Position-wise-Feed-Forward-Networks"><span class="nav-number">5.</span> <span class="nav-text">Position-wise Feed-Forward Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embeddings-and-Softmax"><span class="nav-number">6.</span> <span class="nav-text">Embeddings and Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-number">6.1.</span> <span class="nav-text">Positional Encoding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Self-Attention"><span class="nav-number">7.</span> <span class="nav-text">Why Self-Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Code"><span class="nav-number">8.</span> <span class="nav-text">Code</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiaoXingyu</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.4.4</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v6.5.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.5.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.5.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script>



  



  










  





  

  

  

  

  

  
  

  

  

  

  

  

  

</body>
</html>
